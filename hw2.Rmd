---
title: "Homework 2"
author: Rachael Latimer, Makayla Whitney
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Part 1: Predicting a Categorical Outcome using Regularized Logistic Regression

```{r data setup}
# Load the following packages needed for modeling in this assignment
  
require(caret)
require(recipes)
require(finalfit)
require(glmnet)

# Import the tweet dataset with embeddings

tweet <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/content/post/hw2/data/hw1_tweet_final.csv',header=TRUE)

# Recipe for the tweet dataset

blueprint_tweet <- recipe(x  = tweet,
                          vars  = colnames(tweet),
                          roles = c('outcome',rep('predictor',772))) %>%
  step_dummy('month',one_hot=TRUE) %>% 
  step_harmonic('day',frequency=1,cycle_size=7, role='predictor') %>%
  step_harmonic('date',frequency=1,cycle_size=31,role='predictor') %>%
  step_harmonic('hour',frequency=1,cycle_size=24,role='predictor') %>%
  step_normalize(paste0('Dim',1:768)) %>%
  step_normalize(c('day_sin_1','day_cos_1',
                   'date_sin_1','date_cos_1',
                   'hour_sin_1','hour_cos_1')) %>%
  step_num2factor(sentiment,
                  transform = function(x) x + 1,
                  levels=c('Negative','Positive'))

  
    # Notice that I explicitly specified role=predictor when using
    # step_harmonic function. This assures that the newly derived sin and cos
    # variables has a defined role.
    # You need to do this otherwise caret::train function breaks.
    # caret_train requires every variable in the recipe to have a role
    
    # You can run the following code and make sure every variable has a defined 
    # role. If you want to experiment, remove the role=predictor argument
    # in the step_harmonic function, create the recipe again, and run the following
    # you will see that the new sin and cos variables have NA in the column role
    # and this breaks the caret::train function later.
  
    # Also, in the last line, we transform the outcome variable 'sentiment' to 
    # a factor with labels. 
    # This seems necessary for fitting logistic regression via caret::train

    View(blueprint_tweet %>% prep() %>% summary)


```

##Task 1.1. 
###Split the original data into two subsets: training and test. Let the training data have the 80% of cases and the test data have the 20% of the cases.

```{r 1.1, echo=FALSE}

loc      <- sample(1:nrow(tweet), round(nrow(tweet) * 0.8))
#training dataset
tweet_tr  <- tweet[loc, ]
#testing dataset
tweet_te  <- tweet[-loc, ]

```

##Task 1.2. 
###Use the caret::train() function to train a model with 10-fold cross-validation for predicting the probability of sentiment being positive using logistic regression without any regularization. Evaluate and report the performance of the model on the test dataset.
```{r 1.2}

set.seed(11142021) # for reproducibility

tweet_tr = tweet_tr[sample(nrow(tweet_tr)),]

# Create 10 folds with equal size

folds = cut(seq(1,nrow(tweet_tr)),breaks=10,labels=FALSE)

# Create the list for each fold 

my.indices <- vector('list',10)
for(i in 1:10){
  my.indices[[i]] <- which(folds!=i)
}


cv <- trainControl(method    = "cv",
                   index           = my.indices,
                   classProbs      = TRUE,
                   summaryFunction = mnLogLoss)

# Train the model

caret_mod <- caret::train(blueprint_tweet, 
                          data      = tweet_tr, 
                          method    = "glm",
                          family    = 'binomial',
                          metric    = 'logLoss',
                          trControl = cv)

caret_mod

#logLoss 

nr_ll_te <- 9.362275

# Predict the probabilities for the observations in the test dataset

predicted_te <- predict(caret_mod, tweet_te, type='prob')

dim(predicted_te)
head(predicted_te)

#Calculate the AUC
#install.packages("cutpointr")
require(cutpointr)

cut.obj <- cutpointr(x     = predicted_te$Positive,
                     class = tweet_te$sentiment)

nr_auc_te <- auc(cut.obj)
#0.698213

# Confusion matrix assuming the threshold is 0.5 for evaluation metric calculations

pred_class <- ifelse(predicted_te$Positive>.5,1,0)

confusion <- table(tweet_te$sentiment,pred_class)

confusion

#Calculate Overall Accuracy
nr_acc_te <- (89+101)/(89+50+60+101)
#0.6333333

#Calculate True Negative Rate
nr_tnr_te <- confusion[1,1]/(confusion[1,1]+confusion[1,2])
#0.6756757

#Calculate False Positive Rate
nr_fpr_te <- confusion[1,2]/(confusion[1,1]+confusion[1,2])
#0.3243243

#Calculate True Positive Rate
nr_tpr_te <- confusion[2,2]/(confusion[2,1]+confusion[2,2])
#0.6578947

#Calculate Precision
nr_pre_te <- confusion[2,2]/(confusion[1,2]+confusion[2,2])
#0.6756757


```

##Task 1.3. 
###Use the caret::train() function to train a model with 10-fold cross-validation for predicting the probability of sentiment being positive using logistic regression with ridge penalty. Try different values of ridge penalty to decide the optimal value. Use logLoss as a metric for optimization. Plot the results, and report the optimal value of ridge penalty.
```{r 1.3}

# Cross-validation settings

cv_ridge <- trainControl(method    = "cv_ridge",
                   index           = my.indices,
                   classProbs      = TRUE,
                   summaryFunction = mnLogLoss)
      
# Hyperparameter tuning grid for ridge penalty (lambda), alpha = 0

grid <- data.frame(alpha = 0, lambda = c(seq(0,.001,.00001),.005,.01,.05,.1)) 
grid

# Train the model
  
caret_logistic_ridge <- caret::train(blueprint_tweet, 
                                     data      = tweet_tr, 
                                     method    = "glmnet",
                                     family    = 'binomial',
                                     metric    = 'logLoss',
                                     trControl = cv_ridge,
                                     tuneGrid  = grid)

caret_logistic_ridge

#plot the results
plot(caret_logistic_ridge)

#Optimal value of ridge
caret_logistic_ridge$bestTune

# Predict the probabilities for the observations in the test dataset

predicted_ridge <- predict(caret_logistic_ridge, tweet_te, type='prob')

dim(predicted_ridge)
head(predicted_ridge)

#Calculate LogLoss

caret_logistic_ridge$bestTune
#lambda   logLoss
# 0.10000  0.4607315  

r_ll_te <- 0.4607315

#Calculate the AUC
#install.packages("cutpointr")
require(cutpointr)

cut.obj <- cutpointr(x     = predicted_ridge$Positive,
                     class = tweet_te$sentiment)

r_auc_te <- auc(cut.obj)
#0.9062055

# Confusion matrix assuming the threshold is 0.5 for evaluation metric calculations

pred_class_ridge <- ifelse(predicted_ridge$Positive>.5,1,0)

confusion_ridge <- table(tweet_te$sentiment,pred_class_ridge)

confusion_ridge

#Calculate Overall Accuracy
r_acc_te <- (109+125)/(109+30+36+125)
#0.78

#Calculate True Negative Rate
r_tnr_te <- confusion_ridge[1,1]/(confusion_ridge[1,1]+confusion_ridge[1,2])
#0.8175676

#Calculate True Positive Rate
r_tpr_te <- confusion_ridge[2,2]/(confusion_ridge[2,1]+confusion_ridge[2,2])
#0.8552632

#Calculate Precision
r_pre_te <- confusion_ridge[2,2]/(confusion_ridge[1,2]+confusion_ridge[2,2])
#0.8280255


```

##Task 1.4. 
###Use the caret::train() function to train a model with 10-fold cross-validation for predicting the probability of sentiment being positive using logistic regression with lasso penalty. Try different values of lasso penalty to decide optimal value. Use logLoss as a metric for optimization. Plot the results, and report the optimal value of lasso penalty.
```{r 1.4}

# Cross-validation settings

cv_lasso <- trainControl(method    = "cv_lasso",
                   index           = my.indices,
                   classProbs      = TRUE,
                   summaryFunction = mnLogLoss)
      
# Hyperparameter tuning grid for ridge penalty (lambda), alpha = 0

grid_ <- data.frame(alpha = 0, lambda = 0)
grid_

# Train the model
  
caret_logistic_lasso <- caret::train(blueprint_tweet, 
                                     data      = tweet_tr, 
                                     method    = "glmnet",
                                     family    = 'binomial',
                                     metric    = 'logLoss',
                                     trControl = cv_lasso,
                                     tuneGrid  = grid_)

caret_logistic_lasso

#plot the results
plot(caret_logistic_lasso)

#optimal value of lasso penalty
caret_logistic_lasso$bestTune

# Predict the probabilities for the observations in the test dataset

predicted_lasso <- predict(caret_logistic_lasso, tweet_te, type='prob')

dim(predicted_lasso)
head(predicted_lasso)

#Calculate LogLoss
l_ll_te <- 0.5876427

#Calculate the AUC
#install.packages("cutpointr")
require(cutpointr)

cut.obj <- cutpointr(x     = predicted_lasso$Positive,
                     class = tweet_te$sentiment)

l_auc_te <- auc(cut.obj)
#0.8906028

# Confusion matrix assuming the threshold is 0.5 for evaluation metric calculations

pred_class_lasso <- ifelse(predicted_lasso$Positive>.5,1,0)

confusion_lasso <- table(tweet_te$sentiment,pred_class_lasso)

confusion_lasso

#Calculate Overall Accuracy
l_acc_te <- (104+124)/(104+35+37+124)
#0.76

#Calculate True Negative Rate
l_tnr_te <- confusion_lasso[1,1]/(confusion_lasso[1,1]+confusion_lasso[1,2])
#0.7837838

#Calculate True Positive Rate
l_tpr_te <- confusion_lasso[2,2]/(confusion_lasso[2,1]+confusion_lasso[2,2])
#0.8355263

#Calculate Precision
l_pre_te <- confusion_lasso[2,2]/(confusion_lasso[1,2]+confusion_lasso[2,2])
#0.7987421


```

##Task 1.5 
###Evaluate the performance of the models in 1.2, 1.3, and 1.4 on the test dataset. Calculate and report logLoss (LL), area under the reciever operating characteristic curver (AUC), overall accuracy (ACC), true positive rate (TPR), true negative rate (TNR), and precision (PRE) for three models. When calculating ACC, TPR, TNR, and PRE, assume that we use a cut-off value of 0.5 for the predicted probabilities. Summarize these numbers in a table like the following. Decide and comment on which model you would use to predict sentiment of a tweet moving forward.
```{r 1.5}

logreg <- data.frame(Model = c("Logistic Regression"),
                    LL = c(nr_ll_te),
                     AUC = c(nr_auc_te),
                     ACC = c(nr_acc_te),
                    TPR = c(nr_tpr_te),
                    TNR = c(nr_tnr_te),
                    PRE = c(nr_pre_te))

ridgelogreg <- data.frame(Model = c("Logistic Regression with Ridge Penalty"),
                    LL = c(r_ll_te),
                     AUC = c(r_auc_te),
                     ACC = c(r_acc_te),
                    TPR = c(r_tpr_te),
                    TNR = c(r_tnr_te),
                    PRE = c(r_pre_te))

lassologreg <- data.frame(Model = c("Logistic Regression with Lasso Penalty"),
                    LL = c(l_ll_te),
                     AUC = c(l_auc_te),
                     ACC = c(l_acc_te),
                    TPR = c(l_tpr_te),
                    TNR = c(l_tnr_te),
                    PRE = c(l_pre_te))

#Final Table
EvalTable <- rbind(logreg, ridgelogreg, lassologreg)
EvalTable

```

##Task 1.6 
###For the model you decided in 1.5, find and report the most important 10 predictors of sentiment and their coefficients. Briefly comment which variables seem to be the most important predictors.
```{r 1.6}

#using Logistic Regression with Ridge Penalty 

coefs <- coef(caret_logistic_ridge$finalModel,
              caret_logistic_ridge$bestTune$lambda)

ind   <- order(abs(coefs[,1]),decreasing=T)

head(as.matrix(coefs[ind,]),10)

#The most important predictors include the months between April and June, the time of day,
#and Dim105 and Dim311
```

##Task 1.7. 
###Below are the two tweets I picked from my timeline. Use the model you decided in Task 1.5 to predict a probability that the sentiment being positive for these tweets. You are welcome to extract the word embeddings for these tweets by yourself (model: roberta-base, layer=12). Assume that all these tweets are posted on Saturday, May 1, 2021 at 12pm. For convenience, you can also download the dataset from the link below in case you have trouble in extracting the word embeddings.
```{r 1.7}
new_tweets <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/content/post/hw2/data/toy_tweet_embeddings.csv',header=TRUE)

#assign values for day, month, date, hour
day2 <- c(7,7)
month2 <- c("May", "May")
date2 <-c(1,1)
hour2 <-c(12, 12)

new_tweets2<-cbind(day2, month2, date2, hour2, new_tweets)

predicted_sentiment <- predict(caret_logistic_ridge, new_tweets2, type='prob')
predicted_sentiment

#Both tweets are predicted to be negative.
```

##Task 1.8. 
###Let’s do an experiment and test whether or not the model is biased against certain groups when detecting sentiment of a given text. Below you will find 10 hypothetical tweets with an identical structure. The only thing that changes from tweet to tweet is the subject.
###You are welcome to extract the word embeddings for these tweets by yourself (model: roberta-base, layer=12). Assume that all these tweets are posted on Saturday, May 1, 2021 at 12pm. For convenience, you can also download the dataset from the link below in case you have trouble in extracting the word embeddings.
###Use your model from Task 1.5 to predict the probability of these hypothetical tweets having a positive sentiment, and report these numbers in a table.
```{1.8}




```

```{1.9}
What do you think? Does your model favor any group or seem to be biased against any group? Provide a brief commentary (not more than 500 words).

```

#Part 2: Predicting a Continous Outcome using Regularized Linear Regression

```{r load data}
# Load the following packages needed for modeling in this assignment
  
require(caret)
require(recipes)
require(finalfit)
require(glmnet)

# Import the oregon dataset

oregon <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/content/post/hw2/data/hw1_oregon_final.csv',header=TRUE)

# Recipe for the oregon dataset

  outcome <- 'score'
  
  id      <- 'id'

  categorical <- c('sex','ethnic_cd','tst_bnch','migrant_ed_fg','ind_ed_fg',
                   'sp_ed_fg','tag_ed_fg','econ_dsvntg','stay_in_dist',
                   'stay_in_schl','dist_sped','trgt_assist_fg',
                   'ayp_dist_partic','ayp_schl_partic','ayp_dist_prfrm',
                   'ayp_schl_prfrm','rc_dist_partic','rc_schl_partic',
                   'rc_dist_prfrm','rc_schl_prfrm','grp_rpt_dist_partic',
                   'grp_rpt_schl_partic','grp_rpt_dist_prfrm',
                   'grp_rpt_schl_prfrm')

  numeric <- c('enrl_grd')

  cyclic <- c('date','month')


blueprint_oregon <- recipe(x     = oregon,
                    vars  = c(outcome,categorical,numeric,cyclic),
                    roles = c('outcome',rep('predictor',27))) %>%
  step_indicate_na(all_of(categorical),all_of(numeric)) %>%
  step_zv(all_numeric()) %>%
  step_impute_mean(all_of(numeric)) %>%
  step_impute_mode(all_of(categorical)) %>%
  step_harmonic('date',frequency=1,cycle_size=31,role='predictor') %>%
  step_harmonic('month',frequency=1,cycle_size=12,role='predictor') %>%
  step_ns('enrl_grd',deg_free=3) %>%
  step_normalize(c(paste0(numeric,'_ns_1'),paste0(numeric,'_ns_2'),paste0(numeric,'_ns_3'))) %>%
  step_normalize(c("date_sin_1","date_cos_1","month_sin_1","month_cos_1")) %>%
  step_dummy(all_of(categorical),one_hot=TRUE) %>%
  step_rm(c('date','month'))
    
  View(blueprint_oregon %>% prep() %>% summary)

```

##Task 2.1. 
###Check the dataset for missingness. If there is any variable with more than 75% missingness, remove these variables.
```{r 2.1}

missing <- ff_glimpse(oregon)$Continuous

head(missing)

#flagging missing variables 

flag_na <- which(as.numeric(missing$missing_percent) > 75)
flag_na

# Remove the flagged variables with more than 75% missingness
#no need since there are no variables that meet this criteria


```

##Task 2.2. 
###Split the original data into two subsets: training and test. Let the training data have the 80% of cases and the test data have the 20% of the cases.
```{r 2.2}

set.seed(11152021)  # for reproducibility
  
loc      <- sample(1:nrow(oregon), round(nrow(oregon) * 0.8))
ordata_train  <- oregon[loc, ]
ordata_test  <- oregon[-loc, ]

#train the blueprint
#RL note: I think we don't need prepare when we are using caret
#prepare <- prep(blueprint_oregon, 
#                training = ordata_train)
#prepare

```

##Task 2.3. 
###Use the caret::train() function to train a model with 10-fold cross-validation to predict the scores using linear regression without any regularization. Evaluate the performance of the model on both training and test datasets. Evaluate and report RMSE, R-square, and MAE for both training and test datasets. Is there any evidence of overfitting?
```{r 2.3}

# Randomly shuffle the data

ordata_train = ordata_train[sample(nrow(ordata_train)),]

# Create 10 folds with equal size

folds = cut(seq(1,nrow(ordata_train)),breaks=10,labels=FALSE)

# Create the list for each fold 

my.indices <- vector('list',10)
for(i in 1:10){
  my.indices[[i]] <- which(folds!=i)
}

cv <- trainControl(method = "cv",
                   index  = my.indices)

caret_mod <- caret::train(blueprint_oregon, 
                          data      = ordata_train, 
                          method    = "lm", 
                          trControl = cv)

caret_mod
#   RMSE      Rsquared   MAE     
#  89.67962  0.4009841  69.53005

predicted_te <- predict(caret_mod, ordata_test)

nr_rsq_te <- cor(ordata_test$score,predicted_te)^2
nr_rsq_te
# 0.4063456

nr_mae_te <- mean(abs(ordata_test$score - predicted_te))
nr_mae_te
#69.3389
nr_rmse_te <- sqrt(mean((ordata_test$score - predicted_te)^2))
nr_rmse_te
#89.11403


#RSQ for training data (40%) similar to RSQ for testing data (41%), suggesting
# the model is not overfitted to the training data.
```

##Task 2.4. 
###Use the caret::train() function to train a model with 10-fold cross-validation to predict the scores using ridge regression. Try different values of lambda to decide optimal value. Evaluate the performance of the model on the test dataset, and report RMSE, R-square, and MAE. Does ridge regression provide any improvement over linear regression with no regularization?
```{r 2.4}

grid <- data.frame(alpha = 0, lambda = seq(0.01,3,.01)) 
grid


# Train the model

ridge <- caret::train(blueprint_oregon, 
                      data      = ordata_train, 
                      method    = "glmnet", 
                      trControl = cv,
                      tuneGrid  = grid)

ridge$bestTune

#getting errors in this section
predict_te_ridge <- predict(ridge, ordata_test)

r_rsq_te <- cor(ordata_test$score,predict_te_ridge)^2
r_rsq_te
# 0.4062631
r_mae_te <- mean(abs(ordata_test$score - predict_te_ridge))
r_mae_te
# 69.35026
r_rmse_te <- sqrt(mean((ordata_test$score - predict_te_ridge)^2))
r_rmse_te
# 89.12628  


#Ridge regression produced very similar results as regression with no regularization.
```

##Task 2.5. 
###Use the caret::train() function to train a model with 10-fold cross-validation to predict the scores using lasso regression. Try different values of lambda to decide optimal value. Evaluate the performance of the model on the test dataset, and report RMSE, R-square, and MAE. Does lasso regression provide any improvement over linear regression with no regularization?
```{r 2.5}

grid <- data.frame(alpha = 1, lambda = seq(0.001,0.015,.001)) 

grid

# Train the model

lasso <- caret::train(blueprint_oregon, 
                       data      = ordata_train, 
                       method    = "glmnet", 
                       trControl = cv,
                       tuneGrid  = grid)

lasso$bestTune

#getting errors in this section
predict_te_lasso<- predict(lasso, ordata_test)

l_rsq_te <- cor(ordata_test$score,predict_te_lasso)^2
l_rsq_te
#0.4061835
l_mae_te <- mean(abs(ordata_test$score - predict_te_lasso))
l_mae_te
# 69.34796
l_rmse_te <- sqrt(mean((ordata_test$score - predict_te_lasso)^2))
l_rmse_te
# 89.12685


#Lasso regression produced similar results as regression with no regularization.
```

##Task 2.6 
###Evaluate the performance of the models in 2.2, 2.3, and 2.4 on the test dataset. Calculate and report the root mean squared error (RMSE), mean absolute error (MAE), and R-square. Summarize these numbers in a table like the following. Decide and comment on which model you would use to predict scores.
```{r 2.6}

linreg <- data.frame(Model = c("Linear Regression"),
                    RMSE = c(nr_rmse_te),
                     MAE = c(nr_mae_te),
                     Rsq = c(nr_rsq_te))

ridgereg <- data.frame(Model = c("Linear Regression with Ridge Penalty"),
                    RMSE = c(r_rmse_te),
                     MAE = c(r_mae_te),
                     Rsq = c(r_rsq_te))

lassoreg <- data.frame(Model = c("Linear Regression with Lasso Penalty"),
                    RMSE = c(l_rmse_te),
                     MAE = c(l_mae_te),
                     Rsq = c(l_rsq_te))

#Final Table
ModEvalTable <- rbind(linreg, ridgereg, lassoreg)
ModEvalTable

#Explanation of model choice


```

##Task 2.7 
###For the model you decided in 2.6, find and report the most important 10 predictors of test scores and their regression coefficients. Briefly comment which variables seem to be the most important predictors.
```{r 2.7}
#using Linear Regression with Ridge Penalty 
coefs <- coef(ridge$finalModel,ridge$bestTune$lambda)

coefs.zero <- coefs[which(coefs[,1]==0),]
length(coefs.zero)

coefs.nonzero <- coefs[which(coefs[,1]!=0),]
length(coefs.nonzero)

ind   <- order(abs(coefs.nonzero),decreasing=T)
head(as.matrix(coefs.nonzero[ind[-1]]),10)

#The most important predictors seem to be indicators of TAG education, special education, 
# some ethnic codes
```

